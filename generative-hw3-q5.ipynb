{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1568850,"sourceType":"datasetVersion","datasetId":927102}],"dockerImageVersionId":30804,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch  \nimport torchvision.transforms as transforms  \nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport random\nimport os   \nimport pathlib  \nimport time   \nimport datetime  \nimport glob \n\n\nfrom matplotlib import pyplot as plt  \nfrom IPython.display import display \nfrom PIL import Image   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:20.887761Z","iopub.execute_input":"2024-12-15T19:44:20.888131Z","iopub.status.idle":"2024-12-15T19:44:26.476429Z","shell.execute_reply.started":"2024-12-15T19:44:20.888087Z","shell.execute_reply":"2024-12-15T19:44:26.475513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\n# Path to the dataset\nPATH = '/kaggle/input/cityscapes-pix2pix-dataset/' \ntrain_path = os.path.join(PATH, 'train')\nval_path = os.path.join(PATH, 'val')\n\n# Function to count images in a directory\ndef count_images_in_directory(directory):\n    if not os.path.exists(directory):\n        return 0\n    image_files = [f for f in os.listdir(directory) if f.endswith(('.jpg', '.png', '.jpeg'))]\n    return len(image_files)\n\n# Count images in train and validation sets\nnum_train_images = count_images_in_directory(train_path)\nnum_val_images = count_images_in_directory(val_path)\n\n# Print dataset information\nprint(f\"Dataset Information:\")\nprint(f\" - Number of train images: {num_train_images}\")\nprint(f\" - Number of validation images: {num_val_images}\")\n\n# Load a sample image from the train set\nif num_train_images > 0:\n    image_path = os.path.join(train_path, '1.jpg')\n    sample_image = Image.open(image_path)\n\n    # Convert image to tensor\n    transform = transforms.ToTensor()\n    sample_image_tensor = transform(sample_image)\n\n    # Print sample image information\n    print(\"\\nSample Train Image Information:\")\n    print(f\" - Image path: {image_path}\")\n    print(f\" - Image size: {sample_image.size}\")\n    print(f\" - Tensor shape: {sample_image_tensor.shape}\")\nelse:\n    print(\"\\nNo train images found.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:26.478089Z","iopub.execute_input":"2024-12-15T19:44:26.478495Z","iopub.status.idle":"2024-12-15T19:44:26.630692Z","shell.execute_reply.started":"2024-12-15T19:44:26.478463Z","shell.execute_reply":"2024-12-15T19:44:26.629440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def random_crop(input_image, real_image, IMG_HEIGHT, IMG_WIDTH):\n\n    _, H, W = input_image.shape\n\n    if H < IMG_HEIGHT or W < IMG_WIDTH:\n        raise ValueError(\"The requested crop size is larger than the input image size.\")\n\n    top = random.randint(0, H - IMG_HEIGHT)\n    left = random.randint(0, W - IMG_WIDTH)\n\n    input_cropped = input_image[:, top:top+IMG_HEIGHT, left:left+IMG_WIDTH]\n    real_cropped = real_image[:, top:top+IMG_HEIGHT, left:left+IMG_WIDTH]\n\n    return input_cropped, real_cropped","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:26.631929Z","iopub.execute_input":"2024-12-15T19:44:26.632252Z","iopub.status.idle":"2024-12-15T19:44:26.638903Z","shell.execute_reply.started":"2024-12-15T19:44:26.632220Z","shell.execute_reply":"2024-12-15T19:44:26.637776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load(image_file):  \n    # Read the image file  \n    image = Image.open(image_file)  \n\n    # Convert the image to a tensor  \n    transform = transforms.ToTensor() \n    image_tensor = transform(image)  \n \n    w = image_tensor.size(2)  \n    w //= 2 \n    real_image = image_tensor[:, :, :w]   \n    input_image = image_tensor[:, :, w:]  \n\n    # Convert both images to float32 tensors  \n    input_image = input_image.float()  \n    real_image = real_image.float()  \n   \n    return random_crop(input_image, real_image,256,256)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:26.641834Z","iopub.execute_input":"2024-12-15T19:44:26.642844Z","iopub.status.idle":"2024-12-15T19:44:26.654464Z","shell.execute_reply.started":"2024-12-15T19:44:26.642787Z","shell.execute_reply":"2024-12-15T19:44:26.653507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_indices = [100, 101, 102]   \n\n# Create a figure for plotting  \nplt.figure(figsize=(15, 10))   \n\nfor i, index in enumerate(image_indices):  \n    inp, re = load(str(PATH + f'train/{index}.jpg'))  \n\n    # Display input image  \n    plt.subplot(3, 2, 2 * i + 1)   \n    plt.imshow(inp.permute(1, 2, 0)) \n    plt.title(f\"Input Image (Architecture Label) - {index}\")  \n    plt.axis('off')  # Hide axes  \n\n    # Display real image  \n    plt.subplot(3, 2, 2 * i + 2)  \n    plt.imshow(re.permute(1, 2, 0))  \n    plt.title(f\"Real Image - {index}\")  \n    plt.axis('off')   \n\nplt.tight_layout()  \nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:26.656115Z","iopub.execute_input":"2024-12-15T19:44:26.656439Z","iopub.status.idle":"2024-12-15T19:44:27.619813Z","shell.execute_reply.started":"2024-12-15T19:44:26.656401Z","shell.execute_reply":"2024-12-15T19:44:27.618520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the paths to your train and validation directories  \ntrain_path = '/kaggle/input/cityscapes-pix2pix-dataset/train'  \nval_path = '/kaggle/input/cityscapes-pix2pix-dataset/val'  \n\n\nnum_train_images = len([name for name in os.listdir(train_path) if name.endswith(('.jpg'))])   \nnum_val_images = len([name for name in os.listdir(val_path) if name.endswith(('.jpg'))])  \n\nprint(f'Number of training images: {num_train_images}')  \nprint(f'Number of validation images: {num_val_images}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:27.621255Z","iopub.execute_input":"2024-12-15T19:44:27.621623Z","iopub.status.idle":"2024-12-15T19:44:27.631314Z","shell.execute_reply.started":"2024-12-15T19:44:27.621588Z","shell.execute_reply":"2024-12-15T19:44:27.630153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BUFFER_SIZE = 2975\nBATCH_SIZE = 1\n# Each image is 256x256 in size\nIMG_WIDTH = 256\nIMG_HEIGHT = 256","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:27.632811Z","iopub.execute_input":"2024-12-15T19:44:27.633202Z","iopub.status.idle":"2024-12-15T19:44:27.644427Z","shell.execute_reply.started":"2024-12-15T19:44:27.633150Z","shell.execute_reply":"2024-12-15T19:44:27.643288Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Pix2PixDataset(Dataset):\n    def __init__(self, root_dir, split='train', transform=None):\n        self.root_dir = root_dir\n        self.split = split\n        self.image_paths = glob.glob(os.path.join(root_dir, f'{split}/*.jpg'))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        input_image, real_image = load(image_path) # Uses the previously defined load function\n\n\n        if self.transform:\n            # Optionally apply user-defined transforms\n            input_image = self.transform(input_image)\n            real_image = self.transform(real_image)\n\n        return input_image, real_image\n\n\ncommon_transforms = transforms.Compose([\n    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n])\n\n# Create train dataset and loader\ntrain_dataset = Pix2PixDataset(root_dir=PATH, split='train', transform=common_transforms)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n\ntest_dataset = Pix2PixDataset(root_dir=PATH, split='val', transform=common_transforms)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:27.645871Z","iopub.execute_input":"2024-12-15T19:44:27.646224Z","iopub.status.idle":"2024-12-15T19:44:27.671120Z","shell.execute_reply.started":"2024-12-15T19:44:27.646191Z","shell.execute_reply":"2024-12-15T19:44:27.670002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"OUTPUT_CHANNELS = 3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:27.672472Z","iopub.execute_input":"2024-12-15T19:44:27.672783Z","iopub.status.idle":"2024-12-15T19:44:27.677462Z","shell.execute_reply.started":"2024-12-15T19:44:27.672753Z","shell.execute_reply":"2024-12-15T19:44:27.676315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def downsample(in_channels, out_channels, size, apply_batchnorm=True):\n    layers = []\n    padding = (size - 2) // 2 \n\n    # Add Conv2D layer\n    layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=size, stride=2, padding=padding, bias=not apply_batchnorm))\n    nn.init.normal_(layers[-1].weight, 0.0, 0.02)\n    if layers[-1].bias is not None:\n        nn.init.zeros_(layers[-1].bias)\n    \n    # Add normalization only if spatial size > 1x1\n    if apply_batchnorm:\n        layers.append(nn.BatchNorm2d(out_channels))\n    \n    # Add activation\n    layers.append(nn.LeakyReLU(0.2))\n    \n    return nn.Sequential(*layers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:27.681534Z","iopub.execute_input":"2024-12-15T19:44:27.682316Z","iopub.status.idle":"2024-12-15T19:44:27.689573Z","shell.execute_reply.started":"2024-12-15T19:44:27.682279Z","shell.execute_reply":"2024-12-15T19:44:27.688597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inp = torch.randn(3, 256, 256)  # Example input\n\ndown_model = downsample(in_channels=3, out_channels=3, size=4)\n\ninp = inp.unsqueeze(0)\n\ndown_result = down_model(inp)\n\nprint(down_result.shape)  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:27.690827Z","iopub.execute_input":"2024-12-15T19:44:27.691576Z","iopub.status.idle":"2024-12-15T19:44:27.848627Z","shell.execute_reply.started":"2024-12-15T19:44:27.691526Z","shell.execute_reply":"2024-12-15T19:44:27.847413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def upsample(in_channels, out_channels, size, apply_dropout=False):\n    layers = []\n    padding = (size - 2) // 2  # Calculate padding for 'same' convolution\n    \n    layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size=size, stride=2, padding=padding, bias=False))\n    nn.init.normal_(layers[-1].weight, 0.0, 0.02)\n    \n    layers.append(nn.BatchNorm2d(out_channels))\n    \n    if apply_dropout:\n        layers.append(nn.Dropout(0.5))\n    \n    layers.append(nn.ReLU())\n    \n    return nn.Sequential(*layers)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:27.850121Z","iopub.execute_input":"2024-12-15T19:44:27.850475Z","iopub.status.idle":"2024-12-15T19:44:27.857264Z","shell.execute_reply.started":"2024-12-15T19:44:27.850440Z","shell.execute_reply":"2024-12-15T19:44:27.856103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"down_result = torch.randn(1, 3, 128, 128)  \n\nup_model = upsample(in_channels=3, out_channels=3, size=4, apply_dropout=False)\nup_result = up_model(down_result)\n\nprint(up_result.shape) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:27.858881Z","iopub.execute_input":"2024-12-15T19:44:27.860027Z","iopub.status.idle":"2024-12-15T19:44:27.940469Z","shell.execute_reply.started":"2024-12-15T19:44:27.859976Z","shell.execute_reply":"2024-12-15T19:44:27.939397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        # Downsampling stack\n        self.down_stack = nn.ModuleList([\n            downsample(3, 64, 4, apply_batchnorm=False),  \n            downsample(64, 128, 4),                      \n            downsample(128, 256, 4),                     \n            downsample(256, 512, 4),                     \n            downsample(512, 512, 4),                     \n            downsample(512, 512, 4),                     \n            downsample(512, 512, 4),                     \n            downsample(512, 512, 4, apply_batchnorm=False),  \n        ])\n\n        # Upsampling stack remains unchanged\n        self.up_stack = nn.ModuleList([\n            upsample(512, 512, 4, apply_dropout=True),   \n            upsample(1024, 512, 4, apply_dropout=True),  \n            upsample(1024, 512, 4, apply_dropout=True),  \n            upsample(1024, 512, 4),                      \n            upsample(1024, 256, 4),                      \n            upsample(512, 128, 4),                       \n            upsample(256, 64, 4),                       \n        ])\n\n        # Final layer\n        self.last = nn.ConvTranspose2d(128, OUTPUT_CHANNELS, kernel_size=4, stride=2, padding=1)\n        nn.init.normal_(self.last.weight, 0.0, 0.02)\n        if self.last.bias is not None:\n            nn.init.zeros_(self.last.bias)\n\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        # Downsampling\n        skips = []\n        for down in self.down_stack:\n            x = down(x)\n            skips.append(x)\n        \n        skips = skips[:-1][::-1]  # Reverse skips except the last layer\n\n        # Upsampling\n        for i, up in enumerate(self.up_stack):\n            x = up(x)\n            if i < len(skips):  # Skip connections\n                skip = skips[i]\n                x = torch.cat([x, skip], dim=1)\n\n        x = self.last(x)\n        x = self.tanh(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:27.942255Z","iopub.execute_input":"2024-12-15T19:44:27.942866Z","iopub.status.idle":"2024-12-15T19:44:27.954500Z","shell.execute_reply.started":"2024-12-15T19:44:27.942812Z","shell.execute_reply":"2024-12-15T19:44:27.953275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gen = Generator()\ninp = torch.randn(1, 3, 256, 256)  # Example input: batch of 1, RGB image, 256x256\nout = gen(inp)\nprint(out.shape)  # Should be [1, 3, 256, 256]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:34.994312Z","iopub.execute_input":"2024-12-15T19:44:34.995127Z","iopub.status.idle":"2024-12-15T19:44:36.257470Z","shell.execute_reply.started":"2024-12-15T19:44:34.995083Z","shell.execute_reply":"2024-12-15T19:44:36.256276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LAMBDA = 100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:36.259317Z","iopub.execute_input":"2024-12-15T19:44:36.259701Z","iopub.status.idle":"2024-12-15T19:44:36.264820Z","shell.execute_reply.started":"2024-12-15T19:44:36.259656Z","shell.execute_reply":"2024-12-15T19:44:36.263758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generator_loss(disc_generated_output, gen_output, target, lambda_l1=100):\n  \n    gan_loss = F.binary_cross_entropy_with_logits(disc_generated_output, torch.ones_like(disc_generated_output))\n\n    # L1 loss: Mean absolute error between generated and target images\n    l1_loss = F.l1_loss(gen_output, target)\n\n    # Total generator loss\n    total_gen_loss = gan_loss + lambda_l1 * l1_loss\n\n    return total_gen_loss, gan_loss, l1_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:37.632955Z","iopub.execute_input":"2024-12-15T19:44:37.633403Z","iopub.status.idle":"2024-12-15T19:44:37.639660Z","shell.execute_reply.started":"2024-12-15T19:44:37.633334Z","shell.execute_reply":"2024-12-15T19:44:37.638384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def downsample(in_channels, out_channels, kernel_size, apply_batchnorm=True):\n    layers = []\n    layers.append(\n        nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=2,\n            padding=1,  \n            bias=not apply_batchnorm,\n        )\n    )\n    nn.init.normal_(layers[-1].weight, 0.0, 0.02)\n    if layers[-1].bias is not None:\n        nn.init.zeros_(layers[-1].bias)\n\n    if apply_batchnorm:\n        layers.append(nn.BatchNorm2d(out_channels))\n\n    layers.append(nn.LeakyReLU(0.2))\n\n    return nn.Sequential(*layers)\n\n\n# PatchGAN Discriminator\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        # Downsampling layers\n        self.down1 = downsample(6, 64, 4, apply_batchnorm=False)  \n        self.down2 = downsample(64, 128, 4)\n        self.down3 = downsample(128, 256, 4)\n\n        # Zero padding + Conv2D\n        self.zero_pad1 = nn.ZeroPad2d(1)  \n        self.conv = nn.Conv2d(256, 512, kernel_size=4, stride=1, bias=False)\n        nn.init.normal_(self.conv.weight, 0.0, 0.02)\n\n        self.batchnorm1 = nn.BatchNorm2d(512)\n        self.leaky_relu = nn.LeakyReLU(0.2)\n\n        # Zero padding + Last Conv2D\n        self.zero_pad2 = nn.ZeroPad2d(1)\n        self.last = nn.Conv2d(512, 1, kernel_size=4, stride=1)  \n        nn.init.normal_(self.last.weight, 0.0, 0.02)\n        if self.last.bias is not None:\n            nn.init.zeros_(self.last.bias)\n\n    def forward(self, input_image, target_image):\n        # Concatenate input and target images along the channel dimension\n        x = torch.cat([input_image, target_image], dim=1)  \n\n        x = self.down1(x)  \n        x = self.down2(x)  \n        x = self.down3(x)  \n\n        x = self.zero_pad1(x)  \n        x = self.conv(x)       \n        x = self.batchnorm1(x)\n        x = self.leaky_relu(x)\n\n        x = self.zero_pad2(x)  \n        x = self.last(x)       \n\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:38.757091Z","iopub.execute_input":"2024-12-15T19:44:38.757591Z","iopub.status.idle":"2024-12-15T19:44:38.769731Z","shell.execute_reply.started":"2024-12-15T19:44:38.757549Z","shell.execute_reply":"2024-12-15T19:44:38.768685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def discriminator_loss(disc_real_output, disc_generated_output):\n    \n    real_loss = F.binary_cross_entropy_with_logits(disc_real_output, torch.ones_like(disc_real_output))\n\n    generated_loss = F.binary_cross_entropy_with_logits(disc_generated_output, torch.zeros_like(disc_generated_output))\n\n    total_disc_loss = real_loss + generated_loss\n\n    return total_disc_loss, real_loss, generated_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:39.893098Z","iopub.execute_input":"2024-12-15T19:44:39.893492Z","iopub.status.idle":"2024-12-15T19:44:39.899923Z","shell.execute_reply.started":"2024-12-15T19:44:39.893458Z","shell.execute_reply":"2024-12-15T19:44:39.898711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generator = Generator() \ndiscriminator = Discriminator()  \n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngenerator = generator.to(device)\ndiscriminator = discriminator.to(device)\n\ngenerator_optimizer = optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\ndiscriminator_optimizer = optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:48:07.657179Z","iopub.execute_input":"2024-12-15T19:48:07.657590Z","iopub.status.idle":"2024-12-15T19:48:08.588160Z","shell.execute_reply.started":"2024-12-15T19:48:07.657553Z","shell.execute_reply":"2024-12-15T19:48:08.587077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_images(input_image, target_image, generated_image, epoch, batch_idx):\n    plt.figure(figsize=(15, 15))\n\n    display_list = [\n        input_image[0].permute(1, 2, 0).cpu().numpy(), \n        target_image[0].permute(1, 2, 0).cpu().numpy(), \n        generated_image[0].permute(1, 2, 0).cpu().numpy() \n    ]\n    titles = ['Input Image', 'Segmented Target', 'Generated Image']\n\n    for i in range(3):\n        plt.subplot(1, 3, i + 1)\n        plt.title(titles[i])\n        plt.imshow(display_list[i])\n        plt.axis('off')\n    plt.suptitle(f\"Epoch {epoch}, Batch {batch_idx}\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:41.870290Z","iopub.execute_input":"2024-12-15T19:44:41.870700Z","iopub.status.idle":"2024-12-15T19:44:41.878486Z","shell.execute_reply.started":"2024-12-15T19:44:41.870664Z","shell.execute_reply":"2024-12-15T19:44:41.877303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_step(generator, discriminator, generator_optimizer, discriminator_optimizer, \n               input_image, target, loss_accumulator):\n  \n    # Set models to training mode\n    generator.train()\n    discriminator.train()\n\n    # Enable gradient tracking\n    with torch.set_grad_enabled(True):\n        gen_output = generator(input_image)  \n\n        # Detach generator output for discriminator's backward pass\n        gen_output_detached = gen_output.detach()\n\n        # Discriminator forward passes\n        disc_real_output = discriminator(input_image, target)  \n        disc_generated_output = discriminator(input_image, gen_output_detached)  \n\n        # Compute discriminator loss\n        disc_loss, _, _ = discriminator_loss(disc_real_output, disc_generated_output)\n\n        # Backpropagation for discriminator\n        discriminator_optimizer.zero_grad()\n        disc_loss.backward()  \n        discriminator_optimizer.step()\n\n        # === Update Generator ===\n        disc_generated_output = discriminator(input_image, gen_output)  \n\n        # Compute generator loss\n        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n\n        # Backpropagation for generator\n        generator_optimizer.zero_grad()\n        gen_total_loss.backward()  \n        generator_optimizer.step()\n\n    # Accumulate losses\n    loss_accumulator[\"gen_total_loss\"].append(gen_total_loss.item())\n    loss_accumulator[\"gen_gan_loss\"].append(gen_gan_loss.item())\n    loss_accumulator[\"gen_l1_loss\"].append(gen_l1_loss.item())\n    loss_accumulator[\"disc_loss\"].append(disc_loss.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:42.992949Z","iopub.execute_input":"2024-12-15T19:44:42.994199Z","iopub.status.idle":"2024-12-15T19:44:43.002004Z","shell.execute_reply.started":"2024-12-15T19:44:42.994155Z","shell.execute_reply":"2024-12-15T19:44:43.000936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validation_step(generator, discriminator, input_image, target, loss_accumulator):\n    \"\"\"\n    Perform validation step to compute generator and discriminator losses.\n    \"\"\"\n    # Set models to evaluation mode\n    generator.eval()\n    discriminator.eval()\n\n    with torch.no_grad():\n        # Forward pass for generator\n        gen_output = generator(input_image)\n\n        # Forward passes for discriminator\n        disc_real_output = discriminator(input_image, target)\n        disc_generated_output = discriminator(input_image, gen_output)\n\n        # Compute losses\n        disc_loss, _, _ = discriminator_loss(disc_real_output, disc_generated_output)\n        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n\n        # Accumulate losses\n        loss_accumulator[\"gen_total_loss\"].append(gen_total_loss.item())\n        loss_accumulator[\"gen_gan_loss\"].append(gen_gan_loss.item())\n        loss_accumulator[\"gen_l1_loss\"].append(gen_l1_loss.item())\n        loss_accumulator[\"disc_loss\"].append(disc_loss.item())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T19:44:44.180633Z","iopub.execute_input":"2024-12-15T19:44:44.181647Z","iopub.status.idle":"2024-12-15T19:44:44.188542Z","shell.execute_reply.started":"2024-12-15T19:44:44.181604Z","shell.execute_reply":"2024-12-15T19:44:44.187319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 50\ntrain_loss_history = {\n    \"gen_total_loss\": [],\n    \"gen_gan_loss\": [],\n    \"gen_l1_loss\": [],\n    \"disc_loss\": []\n}\nval_loss_history = {\n    \"gen_total_loss\": [],\n    \"gen_gan_loss\": [],\n    \"gen_l1_loss\": [],\n    \"disc_loss\": []\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(1, num_epochs + 1):\n    print(f\"Epoch {epoch}/{num_epochs}\")\n    \n    # Initialize loss accumulators for training and validation\n    train_loss_accumulator = {\n        \"gen_total_loss\": [],\n        \"gen_gan_loss\": [],\n        \"gen_l1_loss\": [],\n        \"disc_loss\": []\n    }\n    val_loss_accumulator = {\n        \"gen_total_loss\": [],\n        \"gen_gan_loss\": [],\n        \"gen_l1_loss\": [],\n        \"disc_loss\": []\n    }\n\n    # Training phase\n    for step, (input_image, target_image) in enumerate(train_loader):\n        input_image = input_image.to(device)\n        target_image = target_image.to(device)\n\n        train_step(generator, discriminator, generator_optimizer, discriminator_optimizer,\n                   input_image, target_image, train_loss_accumulator)\n\n    # Validation phase\n    for step, (val_input_image, val_target_image) in enumerate(test_loader):\n        val_input_image = val_input_image.to(device)\n        val_target_image = val_target_image.to(device)\n\n        validation_step(generator, discriminator, val_input_image, val_target_image, val_loss_accumulator)\n\n    # Compute average training losses for the epoch\n    avg_train_gen_total_loss = sum(train_loss_accumulator[\"gen_total_loss\"]) / len(train_loss_accumulator[\"gen_total_loss\"])\n    avg_train_gen_gan_loss = sum(train_loss_accumulator[\"gen_gan_loss\"]) / len(train_loss_accumulator[\"gen_gan_loss\"])\n    avg_train_gen_l1_loss = sum(train_loss_accumulator[\"gen_l1_loss\"]) / len(train_loss_accumulator[\"gen_l1_loss\"])\n    avg_train_disc_loss = sum(train_loss_accumulator[\"disc_loss\"]) / len(train_loss_accumulator[\"disc_loss\"])\n\n    # Compute average validation losses for the epoch\n    avg_val_gen_total_loss = sum(val_loss_accumulator[\"gen_total_loss\"]) / len(val_loss_accumulator[\"gen_total_loss\"])\n    avg_val_gen_gan_loss = sum(val_loss_accumulator[\"gen_gan_loss\"]) / len(val_loss_accumulator[\"gen_gan_loss\"])\n    avg_val_gen_l1_loss = sum(val_loss_accumulator[\"gen_l1_loss\"]) / len(val_loss_accumulator[\"gen_l1_loss\"])\n    avg_val_disc_loss = sum(val_loss_accumulator[\"disc_loss\"]) / len(val_loss_accumulator[\"disc_loss\"])\n\n    # Append to global loss history\n    train_loss_history[\"gen_total_loss\"].append(avg_train_gen_total_loss)\n    train_loss_history[\"gen_gan_loss\"].append(avg_train_gen_gan_loss)\n    train_loss_history[\"gen_l1_loss\"].append(avg_train_gen_l1_loss)\n    train_loss_history[\"disc_loss\"].append(avg_train_disc_loss)\n\n    val_loss_history[\"gen_total_loss\"].append(avg_val_gen_total_loss)\n    val_loss_history[\"gen_gan_loss\"].append(avg_val_gen_gan_loss)\n    val_loss_history[\"gen_l1_loss\"].append(avg_val_gen_l1_loss)\n    val_loss_history[\"disc_loss\"].append(avg_val_disc_loss)\n\n    # Print losses for the epoch\n    print(f\"Epoch {epoch} Training Losses:\")\n    print(f\"  Generator Total Loss: {avg_train_gen_total_loss:.4f}\")\n    print(f\"  Generator GAN Loss: {avg_train_gen_gan_loss:.4f}\")\n    print(f\"  Generator L1 Loss: {avg_train_gen_l1_loss:.4f}\")\n    print(f\"  Discriminator Loss: {avg_train_disc_loss:.4f}\")\n\n    print(f\"Epoch {epoch} Validation Losses:\")\n    print(f\"  Generator Total Loss: {avg_val_gen_total_loss:.4f}\")\n    print(f\"  Generator GAN Loss: {avg_val_gen_gan_loss:.4f}\")\n    print(f\"  Generator L1 Loss: {avg_val_gen_l1_loss:.4f}\")\n    print(f\"  Discriminator Loss: {avg_val_disc_loss:.4f}\")\n\n    if epoch % 10 == 0:\n        generator.eval()\n        with torch.no_grad():\n            for batch_idx, (test_input, test_target) in enumerate(test_loader):\n                test_input = test_input.to(device)\n                test_target = test_target.to(device)\n\n                generated_image = generator(test_input)\n                \n                display_images(test_input, test_target, generated_image, epoch, batch_idx)\n                \n                if batch_idx >= 1:\n                    break\nprint(\"Final Results:\")\ngenerator.eval()\nwith torch.no_grad():\n    for batch_idx, (test_input, test_target) in enumerate(test_loader):\n        test_input = test_input.to(device)\n        test_target = test_target.to(device)\n\n        final_generated_image = generator(test_input)\n\n        display_images(test_input, test_target, final_generated_image, \"Final\", batch_idx)\n\n        if batch_idx >= 2:\n            break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Function to plot the loss of Generator and Discriminator over epochs\ndef plot_loss(train_loss_history, val_loss_history):\n    # Extract the number of epochs based on the training history\n    epochs = range(1, len(train_loss_history[\"gen_total_loss\"]) + 1)\n    \n    # Plot Generator Loss\n    plt.figure(figsize=(10, 6))\n    plt.plot(epochs, train_loss_history[\"gen_total_loss\"], label=\"Train Generator Loss\")  # Training loss for Generator\n    plt.plot(epochs, val_loss_history[\"gen_total_loss\"], label=\"Validation Generator Loss\")  # Validation loss for Generator\n    plt.xlabel(\"Epoch\")  # X-axis label\n    plt.ylabel(\"Loss\")  # Y-axis label\n    plt.title(\"Generator Loss Over Epochs\")  # Title of the chart\n    plt.legend()  # Add legend to distinguish between train and validation losses\n    plt.grid()  # Add gridlines for better readability\n    plt.show()  # Display the plot\n\n    # Plot Discriminator Loss\n    plt.figure(figsize=(10, 6))\n    plt.plot(epochs, train_loss_history[\"disc_loss\"], label=\"Train Discriminator Loss\")  # Training loss for Discriminator\n    plt.plot(epochs, val_loss_history[\"disc_loss\"], label=\"Validation Discriminator Loss\")  # Validation loss for Discriminator\n    plt.xlabel(\"Epoch\")  # X-axis label\n    plt.ylabel(\"Loss\")  # Y-axis label\n    plt.title(\"Discriminator Loss Over Epochs\")  # Title of the chart\n    plt.legend()  # Add legend to distinguish between train and validation losses\n    plt.grid()  # Add gridlines for better readability\n    plt.show()  # Display the plot\n\n# Call the function with training and validation loss histories\n# Make sure these dictionaries are populated during the training process\nplot_loss(train_loss_history, val_loss_history)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}